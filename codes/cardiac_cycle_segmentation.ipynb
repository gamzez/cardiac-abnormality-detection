{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction - Cardiac Phase Segmentation\n",
    "This script is segmentating the records into their cardiac phases and extracts statistical features such as mean and standard deviation of a cycle. \n",
    "\n",
    "The processing is as follows:\n",
    "1. PCG records are loaded and chunked.\n",
    "2. Chunks are normalized usng RMS normalization\n",
    "3. Peak detection algorithm is performed on records that were not needed to be manually corrected\n",
    "4. Segmentation results of manually-corrected chunks are loaded\n",
    "5. Segmentation results of all chunks are saved into a single numpy file for easy access.\n",
    "\n",
    "**Note:** *This feature extraction could be also done at the stage of classification however, saving all the features once and accessing them repeatedly by means of loading it is saving time as compared to re-extracting the features.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from python_speech_features import mfcc\n",
    "from python_speech_features import delta\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import scipy\n",
    "from utils import *\n",
    "import random\n",
    "import heartpy as hp\n",
    "from scipy.signal import hilbert, cheby1, filtfilt\n",
    "import numpy as np\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "import wfdb\n",
    "from segmentation_utils import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Loading and RMS Normalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_frequency = 2000 # in Hz\n",
    "slice_length = 4          # in seconds\n",
    "overlap = 2               # in seconds\n",
    "\n",
    "normal_records, normal_records_denoised = load_records(path = '../dataset/normal_tracings.txt',normalize = False, crop = 20000)\n",
    "abnormal_records, abnormal_records_denoised = load_records(path = '../dataset/abnormal_tracings.txt',normalize = False, crop = 20000)\n",
    "records_train = np.concatenate((abnormal_records_denoised, normal_records_denoised), axis=0)\n",
    "labels = np.concatenate((np.ones((abnormal_records_denoised.shape[0],1)),np.zeros((normal_records.shape[0],1))), axis=0)\n",
    "\n",
    "normal_records_chunks = np.zeros((normal_records_denoised.shape[0], 4, slice_length*sampling_frequency))\n",
    "slices = np.arange(0, 10, slice_length-overlap, dtype=int)\n",
    "for i in range(normal_records.shape[0]):\n",
    "    j = 0\n",
    "    for start, end in zip(slices[:-1], slices[1:]):\n",
    "        start_audio = start * sampling_frequency\n",
    "        end_audio = (end + overlap)* sampling_frequency\n",
    "        chunk = normal_records_denoised[i, int(start_audio): int(end_audio)]\n",
    "        normal_records_chunks[i, j, :] = chunk\n",
    "        j = j + 1  \n",
    "        \n",
    "abnormal_records_chunks = np.zeros((abnormal_records_denoised.shape[0], 4, slice_length*sampling_frequency))\n",
    "for i in range(abnormal_records.shape[0]):\n",
    "    j = 0\n",
    "    for start, end in zip(slices[:-1], slices[1:]):\n",
    "        start_audio = start * sampling_frequency\n",
    "        end_audio = (end + overlap)* sampling_frequency\n",
    "        chunk = abnormal_records_denoised[i, int(start_audio): int(end_audio)]\n",
    "        abnormal_records_chunks[i, j, :] = chunk\n",
    "        j = j + 1  \n",
    "\n",
    "abnormal_records_chunks_reshaped = np.reshape(abnormal_records_chunks, (33*4, 8000))\n",
    "normal_records_chunks_reshaped = np.reshape(normal_records_chunks, (29*4, 8000))\n",
    "all_records_chunked = np.concatenate((abnormal_records_chunks_reshaped, normal_records_chunks_reshaped), axis = 0)\n",
    "all_records_chunked_normalized = np.zeros_like(all_records_chunked)\n",
    "labels_chunked = np.concatenate((np.ones((abnormal_records_chunks_reshaped.shape[0],1)),np.zeros((normal_records_chunks_reshaped.shape[0],1))), axis=0)\n",
    "\n",
    "\n",
    "for i in range(all_records_chunked.shape[0]):\n",
    "    data = all_records_chunked[i,:]\n",
    "    rms_level = 0\n",
    "    r = 10**(rms_level / 10.0)\n",
    "    a = np.sqrt( (len(data) * r**2) / np.sum(data**2) )\n",
    "    data = data * a\n",
    "    all_records_chunked_normalized[i,:] = data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mean_S1 = np.zeros((all_records_chunked_normalized.shape[0],1))\n",
    "all_mean_S2 = np.zeros((all_records_chunked_normalized.shape[0],1))\n",
    "all_std_S1 = np.zeros((all_records_chunked_normalized.shape[0],1))\n",
    "all_std_S2 = np.zeros((all_records_chunked_normalized.shape[0],1))\n",
    "\n",
    "all = np.arange(0,248,1)\n",
    "\n",
    "corrected = [9,10,20,21,22,23,28,34,57,60,65,66,67,\n",
    "                80,84,85,86,97,101,102,105,106,112,119,\n",
    "                128,129,130,131,162,164,165,166,178,179,\n",
    "                187,194,206,211,228,229,230,231,235,238,\n",
    "                239,240,241,243,68,69,70,71,99,108,110,\n",
    "                111,116,234]  # indices of manually corrected chunks\n",
    "\n",
    "rejection = [68,69,70,71,108,110,111]  # indices of chunks that could not been segmented\n",
    "\n",
    "all_clear = np.delete(all,corrected)\n",
    "\n",
    "for j in range(all_mean_S1.shape[0]):\n",
    "    \n",
    "    if j not in corrected:\n",
    "            \n",
    "        PCG = all_records_chunked_normalized[j,:]\n",
    "        sampling_rate = 2000\n",
    "\n",
    "        PCG_f = cheby1_bandpass_filter(PCG, lowcut=10, highcut=500, fs=sampling_rate, order=4)\n",
    "        dn = (np.append(PCG_f[1:], 0) - PCG_f)\n",
    "        dtn = dn/(np.max(abs(dn)))\n",
    "        an = abs(dtn)\n",
    "        en = an**2\n",
    "        sen = -abs(dtn) * np.log10(abs(dtn))\n",
    "        sn = -(dtn**2) * np.log10(dtn**2)\n",
    "        window_len = 50\n",
    "        sn_f = np.insert(running_mean(sn, window_len), 0, [0] * (window_len - 1))\n",
    "        zn = np.imag(hilbert(sn_f))\n",
    "        ma_len = 4000\n",
    "        zn_ma = np.insert(running_mean(zn, ma_len), 0, [0] * (ma_len - 1))\n",
    "        zn_ma_s = zn - zn_ma\n",
    "\n",
    "        idx = np.argwhere(np.diff(np.sign(zn_ma_s)) > 0).flatten().tolist()\n",
    "        idx_search = []\n",
    "        id_maxes = np.empty(0, dtype=int)\n",
    "        search_window_half = round(sampling_rate * .15)  \n",
    "        for i in idx:\n",
    "            lows = np.arange(i-search_window_half, i)\n",
    "            highs = np.arange(i+1, i+search_window_half+1)\n",
    "            if highs[-1] > len(PCG):\n",
    "                highs = np.delete(highs, np.arange(np.where(highs == len(PCG))[0], len(highs))) \n",
    "            PCG_window = np.concatenate((lows, [i], highs))\n",
    "            idx_search.append(PCG_window)\n",
    "            PCG_window_wave = PCG[PCG_window]\n",
    "            id_maxes = np.append(id_maxes, PCG_window[np.where(PCG_window_wave == np.max(PCG_window_wave))[0]])\n",
    "\n",
    "        id_maxes =  id_maxes[id_maxes >= 0]  # removal of potential negative indices\n",
    "        id_maxes = np.unique(id_maxes)       # removal of potential duplicates\n",
    "        _, ev,od = interval(id_maxes)\n",
    "        min_int = np.minimum(np.mean(ev),np.mean(od))\n",
    "        \n",
    "        id_maxes =  clear_adjacents(id_maxes, min_int) # cleaning adjacent peaks that happened too soon (id_maxes)\n",
    "        _, ev,od = interval(id_maxes)\n",
    "\n",
    "        # depending on which set of intervals are longer, they are assigned either to S1 or S2    \n",
    "        if np.mean(ev) > np.mean(od):\n",
    "            meanS2 = np.mean(ev);\n",
    "            stdS2 = np.std(ev); \n",
    "            meanS1 = np.mean(od);\n",
    "            stdS1 = np.std(od);\n",
    "        else:      \n",
    "            meanS2 = np.mean(od);\n",
    "            stdS2 = np.std(od); \n",
    "            meanS1 = np.mean(ev);\n",
    "            stdS1 = np.std(ev);\n",
    "        \n",
    "        all_mean_S1[j] = meanS1 \n",
    "        all_mean_S2[j] = meanS2 \n",
    "        all_std_S1[j] = stdS1\n",
    "        all_std_S2[j] = stdS2\n",
    "\n",
    "        \n",
    "    if j in corrected and j not in rejection:\n",
    "        directory =  \"cardiac_cycle_segmentation_features/corrected_chunks/chunk_\" + str(j) + \".npy\"\n",
    "        values = np.load(directory)\n",
    "        meanS1 = values[0,0]\n",
    "        meanS2 = values[0,1]\n",
    "        stdS1 = values[0,2]\n",
    "        stdS2 = values[0,3]\n",
    "        all_mean_S1[j] = meanS1 \n",
    "        all_mean_S2[j] = meanS2 \n",
    "        all_std_S1[j] = stdS1\n",
    "        all_std_S2[j] = stdS2\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving segmentation features of all chunks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mean_S1 = np.array(all_mean_S1)\n",
    "all_mean_S2 = np.array(all_mean_S2)\n",
    "all_std_S1 = np.array(all_std_S1)\n",
    "all_std_S2 = np.array(all_std_S2)\n",
    "ratio = all_mean_S1 / all_mean_S2\n",
    "\n",
    "np.save(\"all_mean_S1.npy\", all_mean_S1)\n",
    "np.save(\"all_mean_S2.npy\", all_mean_S2)\n",
    "np.save(\"all_std_S1.npy\", all_std_S1)\n",
    "np.save(\"all_std_S2.npy\", all_std_S2) \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('gamze_thesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7c1921d6cb1bbb6d983fc9ed48edae35d2edb378643e089b074f8a35a8a8a2d1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
